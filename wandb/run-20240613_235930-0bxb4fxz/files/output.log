  0%|                                                                                                                         | 0/192 [00:00<?, ?it/s]/root/miniconda3/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")









  5%|█████▊                                                                                                          | 10/192 [02:12<39:18, 12.96s/it]









 10%|███████████▋                                                                                                    | 20/192 [04:55<39:25, 13.75s/it]
 50%|█████████████████████████████████████████████████████████▌                                                         | 2/4 [00:00<00:00,  4.73it/s]
  warnings.warn(
/root/miniconda3/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'eval_loss': 1.1705877780914307, 'eval_runtime': 1.7611, 'eval_samples_per_second': 18.17, 'eval_steps_per_second': 2.271, 'epoch': 0.31}









 16%|█████████████████▌                                                                                              | 30/192 [07:55<41:32, 15.38s/it]









 21%|███████████████████████▎                                                                                        | 40/192 [10:42<42:30, 16.78s/it]
  0%|                                                                                                                           | 0/4 [00:00<?, ?it/s]

{'loss': 1.0661, 'grad_norm': 0.24553166329860687, 'learning_rate': 0.0002571428571428571, 'epoch': 0.62}
 21%|███████████████████████▎                                                                                        | 40/192 [10:43<42:30, 16.78s/it]/root/miniconda3/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/root/miniconda3/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")









 26%|█████████████████████████████▏                                                                                  | 50/192 [13:33<50:02, 21.14s/it]









 31%|██████████████████████████████████▍                                                                             | 59/192 [15:30<29:40, 13.39s/it]
 31%|███████████████████████████████████                                                                             | 60/192 [15:43<29:13, 13.28s/it]
 75%|██████████████████████████████████████████████████████████████████████████████████████▎                            | 3/4 [00:00<00:00,  3.19it/s]
  warnings.warn(
/root/miniconda3/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")









 36%|████████████████████████████████████████▊                                                                       | 70/192 [18:33<27:32, 13.55s/it]










 42%|██████████████████████████████████████████████▋                                                                 | 80/192 [21:17<26:37, 14.26s/it]

 42%|██████████████████████████████████████████████▋                                                                 | 80/192 [21:19<26:37, 14.26s/it]
 42%|██████████████████████████████████████████████▋                                                                 | 80/192 [21:19<26:37, 14.26s/it]/root/miniconda3/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/root/miniconda3/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")









 47%|████████████████████████████████████████████████████▌                                                           | 90/192 [24:09<26:37, 15.66s/it]









 52%|█████████████████████████████████████████████████████████▊                                                     | 100/192 [27:04<29:13, 19.06s/it]
  0%|                                                                                                                           | 0/4 [00:00<?, ?it/s]

{'loss': 0.9293, 'grad_norm': 1.4048058986663818, 'learning_rate': 0.00016153846153846153, 'epoch': 1.56}
 52%|█████████████████████████████████████████████████████████▊                                                     | 100/192 [27:05<29:13, 19.06s/it]/root/miniconda3/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/root/miniconda3/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")









 57%|███████████████████████████████████████████████████████████████▌                                               | 110/192 [29:54<33:32, 24.55s/it]









 62%|██████████████████████████████████████████████████████████▊                                   | 120/192 [32:05<16:06, 13.43s/it]
 75%|█████████████████████████████████████████████████████████████████████████▌                        | 3/4 [00:00<00:00,  3.34it/s]
  warnings.warn(
/root/miniconda3/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'eval_loss': 0.8653605580329895, 'eval_runtime': 1.6943, 'eval_samples_per_second': 18.887, 'eval_steps_per_second': 2.361, 'epoch': 1.88}









 68%|███████████████████████████████████████████████████████████████▋                              | 130/192 [34:59<14:19, 13.87s/it]









 73%|████████████████████████████████████████████████████████████████████▌                         | 140/192 [37:42<12:25, 14.34s/it]
 50%|█████████████████████████████████████████████████                                                 | 2/4 [00:00<00:00,  4.52it/s]
  warnings.warn(
/root/miniconda3/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'eval_loss': 0.8571768403053284, 'eval_runtime': 1.7512, 'eval_samples_per_second': 18.273, 'eval_steps_per_second': 2.284, 'epoch': 2.19}









 78%|█████████████████████████████████████████████████████████████████████████▍                    | 150/192 [40:30<11:39, 16.66s/it]









 83%|██████████████████████████████████████████████████████████████████████████████▎               | 160/192 [43:20<11:16, 21.13s/it]
 50%|█████████████████████████████████████████████████                                                 | 2/4 [00:00<00:00,  4.76it/s]
{'loss': 0.9003, 'grad_norm': 0.28797265887260437, 'learning_rate': 6.263736263736263e-05, 'epoch': 2.5}
  warnings.warn(
/root/miniconda3/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")









 89%|███████████████████████████████████████████████████████████████████████████████████▏          | 170/192 [45:33<04:59, 13.63s/it]









 93%|███████████████████████████████████████████████████████████████████████████████████████▋      | 179/192 [48:02<02:56, 13.55s/it]
 94%|████████████████████████████████████████████████████████████████████████████████████████▏     | 180/192 [48:15<02:40, 13.33s/it]
  0%|                                                                                                          | 0/4 [00:00<?, ?it/s]
  warnings.warn(
/root/miniconda3/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")








 98%|████████████████████████████████████████████████████████████████████████████████████████████▌ | 189/192 [50:48<00:42, 14.31s/it]


100%|██████████████████████████████████████████████████████████████████████████████████████████████| 192/192 [51:28<00:00, 13.63s/it]Traceback (most recent call last):
  File "finetune.py", line 288, in <module>
    if __name__ == "__main__":
  File "/root/miniconda3/lib/python3.8/site-packages/fire/core.py", line 143, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/root/miniconda3/lib/python3.8/site-packages/fire/core.py", line 477, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File "/root/miniconda3/lib/python3.8/site-packages/fire/core.py", line 693, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "finetune.py", line 278, in train
  File "/root/miniconda3/lib/python3.8/site-packages/transformers/trainer.py", line 1885, in train
    return inner_training_loop(
  File "/root/miniconda3/lib/python3.8/site-packages/transformers/trainer.py", line 2339, in _inner_training_loop
    self._load_best_model()
  File "/root/miniconda3/lib/python3.8/site-packages/transformers/trainer.py", line 2636, in _load_best_model
    model.load_adapter(self.state.best_model_checkpoint, active_adapter)
  File "/root/miniconda3/lib/python3.8/site-packages/peft/peft_model.py", line 1014, in load_adapter
    adapters_weights = load_peft_weights(model_id, device=torch_device, **hf_hub_download_kwargs)
  File "/root/miniconda3/lib/python3.8/site-packages/peft/utils/save_and_load.py", line 475, in load_peft_weights
    adapters_weights = safe_load_file(filename, device=device)
  File "/root/miniconda3/lib/python3.8/site-packages/safetensors/torch.py", line 311, in load_file
    with safe_open(filename, framework="pt", device=device) as f:
safetensors_rust.SafetensorError: Error while deserializing header: InvalidHeaderDeserialization